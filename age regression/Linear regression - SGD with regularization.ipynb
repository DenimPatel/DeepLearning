{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression - SGD with regularization.ipynb\r\n",
      "age_regression_Xte.npy\r\n",
      "age_regression_Xtr.npy\r\n",
      "age_regression_yte.npy\r\n",
      "age_regression_ytr.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_total = np.load(\"age_regression_Xtr.npy\").reshape([-1,48*48]);\n",
    "y_train_total = np.load(\"age_regression_ytr.npy\");\n",
    "x_test = np.load(\"age_regression_Xte.npy\").reshape([-1,48*48]);\n",
    "y_test = np.load(\"age_regression_yte.npy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 80\n",
    "partition = int(len(x_train_total)*(percentage/100))\n",
    "x_train, y_train = x_train_total[:partition,:], y_train_total[:partition,]\n",
    "x_validation, y_validation = x_train_total[partition:,:], y_train_total[partition:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD from scratch\n",
    "lr = 0.00000000001\n",
    "for _ in range(50):\n",
    "    prediction = x_train.dot(w)\n",
    "    MSE = 0.5* np.mean((prediction-y_train)**2)\n",
    "    print(MSE)\n",
    "    grad = np.mean(np.dot((prediction-y_train),x_train))\n",
    "    w = w - lr*grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD from scratch\n",
    "lr = 0.0000001\n",
    "for _ in range(50):\n",
    "    prediction = x_test.dot(w) + b\n",
    "    MSE = 0.5* np.mean((prediction-y_test)**2)\n",
    "    print(MSE)\n",
    "    grad_w = np.mean(np.dot((prediction-y_test),x_test))\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = np.mean(prediction-y_test)\n",
    "    b = b - lr*grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD from scratch\n",
    "lr = 0.0000001\n",
    "for _ in range(50):\n",
    "    prediction = x_test.dot(w) + b\n",
    "    MSE = 0.5* np.mean((prediction-y_test)**2)\n",
    "    print(MSE)\n",
    "    grad_w = np.mean(np.dot((prediction-y_test),x_test))\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = np.mean(prediction-y_test)\n",
    "    b = b - lr*grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random([48*48])\n",
    "b = np.random.rand()\n",
    "model = [w,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, model):\n",
    "    w= model[0]\n",
    "    b= model[1]\n",
    "    y = x.dot(w) + b\n",
    "    return y\n",
    "def calculate_loss(predicted_y, actual_y):\n",
    "    MSE = 0.5* np.mean((predicted_y-actual_y)**2)\n",
    "    return MSE\n",
    "def calculate_regularization(w,alpha):\n",
    "    REGULARIZATION = 0.5 * alpha * np.dot(w,w) \n",
    "    return REGULARIZATION\n",
    "def backprop(prediction, x, y, model, alpha, lr=0.00001):\n",
    "    w = model[0]\n",
    "    b = model[1] \n",
    "    grad_w = np.mean(np.dot((prediction-y),x)) + alpha * w\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = np.mean(prediction-y)\n",
    "    b = b - lr*grad_b\n",
    "    return [w, b]\n",
    "def evaluate(x = x_validation, y = y_validation, model=model, alpha=0.1):\n",
    "    prediction = forward_pass(x, model)\n",
    "    loss = calculate_loss(prediction, y)\n",
    "    w = model[0]\n",
    "    reg_loss = calculate_regularization(w,alpha)\n",
    "    return loss+reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - GRID SEARCH\n",
    "lrs = [3.3e-6, 1e-6, 3.3e-5, 1e-5]\n",
    "alphas = [0.001, 0.01, 0.1, 1]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "epochs = [25, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  25 \t loss:  132.06772484029554\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  50 \t loss:  136.82669565445318\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  75 \t loss:  135.0601540795833\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  100 \t loss:  140.89200967630538\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  25 \t loss:  138.51501949656043\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  50 \t loss:  134.10602593890135\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  75 \t loss:  142.3773813485832\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  100 \t loss:  144.83976503348862\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  25 \t loss:  135.5597401160388\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  50 \t loss:  137.7073094907091\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  75 \t loss:  137.994661493628\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  100 \t loss:  137.97648323045883\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  25 \t loss:  145.9183757665425\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  50 \t loss:  139.56468642058164\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  75 \t loss:  132.60953571258133\n",
      "lr:  3.3e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  100 \t loss:  133.88483978780098\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  25 \t loss:  137.93333402720225\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  50 \t loss:  134.7692670926454\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  75 \t loss:  134.25465310802437\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  100 \t loss:  141.36853698922963\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  25 \t loss:  139.83867150242827\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  50 \t loss:  133.0419856081856\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  75 \t loss:  140.79848527848918\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  100 \t loss:  143.0766887363222\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  25 \t loss:  145.39704012243553\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  50 \t loss:  133.93235045613704\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  75 \t loss:  136.28425639446385\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  100 \t loss:  131.49523699705574\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  25 \t loss:  134.16674251276166\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  50 \t loss:  129.42309467350248\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  75 \t loss:  134.68798227674856\n",
      "lr:  3.3e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  100 \t loss:  140.15454155485432\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  25 \t loss:  145.85845179579215\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  50 \t loss:  151.78620650517524\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  75 \t loss:  145.35917058717243\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  100 \t loss:  144.73687571547805\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  25 \t loss:  152.68156731659212\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  50 \t loss:  144.95472256320778\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  75 \t loss:  146.2397897624304\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  100 \t loss:  151.4912596334591\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  25 \t loss:  148.53885920645484\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  50 \t loss:  153.9223643150072\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  75 \t loss:  150.91583759184908\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  100 \t loss:  150.6921338399709\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  25 \t loss:  143.67994094155085\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  50 \t loss:  147.99957633621995\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  75 \t loss:  141.97055117535783\n",
      "lr:  3.3e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  100 \t loss:  149.69970107033686\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  25 \t loss:  238.6168845912843\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  50 \t loss:  219.55687086210435\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  75 \t loss:  209.96616655266774\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  100 \t loss:  208.5061923733437\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  25 \t loss:  231.30481539638615\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  50 \t loss:  232.5527818997727\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  75 \t loss:  225.65163681538172\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  100 \t loss:  219.570113451211\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  25 \t loss:  233.11081813085326\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  50 \t loss:  225.6742606795329\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  75 \t loss:  224.41738641719235\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  100 \t loss:  222.255985538178\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  25 \t loss:  239.8407726306738\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  50 \t loss:  227.12762362357907\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  75 \t loss:  227.7175123327074\n",
      "lr:  3.3e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  100 \t loss:  235.19257179291816\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  25 \t loss:  146.152776810943\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  50 \t loss:  140.06504166913743\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  75 \t loss:  140.91524296854664\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  8 \t epoch:  100 \t loss:  137.99750372555232\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  25 \t loss:  136.7902514648466\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  50 \t loss:  136.0141134037181\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  75 \t loss:  133.32012040164003\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  16 \t epoch:  100 \t loss:  137.6245658568521\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  25 \t loss:  134.31842142733785\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  50 \t loss:  137.00734873955886\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  75 \t loss:  136.45917326921335\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  32 \t epoch:  100 \t loss:  139.7670326622031\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  25 \t loss:  135.0834098756237\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  50 \t loss:  143.01387448154892\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  75 \t loss:  140.6363316404122\n",
      "lr:  1e-06 \t alpha:  0.001 \t batch_size:  64 \t epoch:  100 \t loss:  134.57052366249295\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  25 \t loss:  146.0825619026199\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  50 \t loss:  147.0745347771715\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  75 \t loss:  140.77703573757404\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  8 \t epoch:  100 \t loss:  134.47715648943276\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  25 \t loss:  144.32046044625608\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  50 \t loss:  140.64274128856414\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  75 \t loss:  136.72308763041153\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  16 \t epoch:  100 \t loss:  140.2126972009534\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  25 \t loss:  139.88123718846043\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  50 \t loss:  136.81523546900334\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  75 \t loss:  140.01357531599865\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  32 \t epoch:  100 \t loss:  139.5234397795499\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  25 \t loss:  136.72813382730826\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  50 \t loss:  134.52254953253126\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  75 \t loss:  137.51746500952845\n",
      "lr:  1e-06 \t alpha:  0.01 \t batch_size:  64 \t epoch:  100 \t loss:  144.4316373638372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  25 \t loss:  145.17302814822182\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  50 \t loss:  145.99149686241324\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  75 \t loss:  145.38730859580852\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  8 \t epoch:  100 \t loss:  151.62769260999173\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  25 \t loss:  149.14530725751376\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  50 \t loss:  144.2386308931811\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  75 \t loss:  141.10662784651944\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  16 \t epoch:  100 \t loss:  151.3546975241398\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  25 \t loss:  147.75365464472793\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  50 \t loss:  146.25316113416562\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  75 \t loss:  136.84159324855779\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  32 \t epoch:  100 \t loss:  146.57119512474563\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  25 \t loss:  149.67269940201578\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  50 \t loss:  150.30193060386145\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  75 \t loss:  143.3942034376633\n",
      "lr:  1e-06 \t alpha:  0.1 \t batch_size:  64 \t epoch:  100 \t loss:  153.45783851254814\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  25 \t loss:  228.8393372758911\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  50 \t loss:  229.58023316566909\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  75 \t loss:  230.26514994890633\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  8 \t epoch:  100 \t loss:  232.46704228072397\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  25 \t loss:  227.48419464086976\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  50 \t loss:  235.3867429211483\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  75 \t loss:  231.2703115936776\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  16 \t epoch:  100 \t loss:  235.98688354438474\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  25 \t loss:  239.84738770728325\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  50 \t loss:  238.52596235683183\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  75 \t loss:  229.6663379253221\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  32 \t epoch:  100 \t loss:  226.74880124902904\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  25 \t loss:  234.8308495425822\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  50 \t loss:  234.35129487118718\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  75 \t loss:  234.8232862290935\n",
      "lr:  1e-06 \t alpha:  1 \t batch_size:  64 \t epoch:  100 \t loss:  233.18394806061653\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  25 \t loss:  139.1040280939654\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  50 \t loss:  138.57117706275244\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  75 \t loss:  140.19883921295522\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  100 \t loss:  132.75436089228253\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  25 \t loss:  140.6408450847823\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  50 \t loss:  137.7256216392876\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  75 \t loss:  141.18155836407743\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  100 \t loss:  132.9658734572163\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  25 \t loss:  142.268250689086\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  50 \t loss:  137.56760985480727\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  75 \t loss:  137.8523646971798\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  100 \t loss:  144.5161269190743\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  25 \t loss:  140.66709214092705\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  50 \t loss:  134.20693081083\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  75 \t loss:  142.78204822399496\n",
      "lr:  3.3e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  100 \t loss:  139.08273511816682\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  25 \t loss:  132.89836549344582\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  50 \t loss:  137.9580501645635\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  75 \t loss:  138.1437622473576\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  100 \t loss:  137.0678800610575\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  25 \t loss:  145.19660646711577\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  50 \t loss:  140.0244748674941\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  75 \t loss:  138.01673890942806\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  100 \t loss:  135.33031967474633\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  25 \t loss:  134.4245786335157\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  50 \t loss:  140.90643512080118\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  75 \t loss:  144.51135168408751\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  100 \t loss:  140.52365495756945\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  25 \t loss:  144.73046204624097\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  50 \t loss:  142.30309601083096\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  75 \t loss:  144.0268903443675\n",
      "lr:  3.3e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  100 \t loss:  139.1445895026474\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  25 \t loss:  145.2811242640529\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  50 \t loss:  147.0677053776879\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  75 \t loss:  139.51580797370025\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  100 \t loss:  142.94727430328064\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  25 \t loss:  144.66735248163167\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  50 \t loss:  143.06322328559747\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  75 \t loss:  149.7057508613982\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  100 \t loss:  142.79285529114696\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  25 \t loss:  148.96431499047955\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  50 \t loss:  151.75254533122964\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  75 \t loss:  146.11270485747056\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  100 \t loss:  138.93157339126694\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  25 \t loss:  145.92867619008763\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  50 \t loss:  142.95131273337705\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  75 \t loss:  148.33698831905429\n",
      "lr:  3.3e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  100 \t loss:  146.20477344899447\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  25 \t loss:  178.97951583105612\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  50 \t loss:  156.95276936613683\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  75 \t loss:  142.7502772318665\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  100 \t loss:  136.54492441474486\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  25 \t loss:  199.16205440426836\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  50 \t loss:  174.82935184050513\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  75 \t loss:  166.52769645728137\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  100 \t loss:  152.1105841792705\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  25 \t loss:  223.85681471591624\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  50 \t loss:  202.12207238001875\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  75 \t loss:  189.39830411924177\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  100 \t loss:  183.99324805709838\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  25 \t loss:  225.593754661182\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  50 \t loss:  213.19416380142638\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  75 \t loss:  207.45951812104113\n",
      "lr:  3.3e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  100 \t loss:  204.17765064537724\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  25 \t loss:  135.3709299677465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  50 \t loss:  132.8730354231779\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  75 \t loss:  147.08814723405905\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  8 \t epoch:  100 \t loss:  132.05448821114763\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  25 \t loss:  134.16985652613923\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  50 \t loss:  142.25042985683123\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  75 \t loss:  138.79403457767344\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  16 \t epoch:  100 \t loss:  144.12520555026944\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  25 \t loss:  139.66768436649355\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  50 \t loss:  133.01904367300898\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  75 \t loss:  138.99414180663766\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  32 \t epoch:  100 \t loss:  137.99364355622\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  25 \t loss:  136.4162224853909\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  50 \t loss:  140.37507094347202\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  75 \t loss:  134.71167901554998\n",
      "lr:  1e-05 \t alpha:  0.001 \t batch_size:  64 \t epoch:  100 \t loss:  139.31525218972683\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  25 \t loss:  136.5422050491594\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  50 \t loss:  138.7001162094358\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  75 \t loss:  133.8302291902188\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  8 \t epoch:  100 \t loss:  146.07135308797845\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  25 \t loss:  134.8802256190386\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  50 \t loss:  139.86796751514456\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  75 \t loss:  138.45934596213718\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  16 \t epoch:  100 \t loss:  141.08327893935825\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  25 \t loss:  134.64581553912433\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  50 \t loss:  148.39049199266933\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  75 \t loss:  135.62935278263393\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  32 \t epoch:  100 \t loss:  135.9745913926165\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  25 \t loss:  137.7820035924086\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  50 \t loss:  139.38972457841803\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  75 \t loss:  135.4495359111876\n",
      "lr:  1e-05 \t alpha:  0.01 \t batch_size:  64 \t epoch:  100 \t loss:  136.0393308823299\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  25 \t loss:  147.06148471843917\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  50 \t loss:  149.02472377295362\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  75 \t loss:  148.56054959350135\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  8 \t epoch:  100 \t loss:  142.5943640257482\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  25 \t loss:  152.10034189651537\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  50 \t loss:  146.50290589571443\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  75 \t loss:  150.85947982324163\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  16 \t epoch:  100 \t loss:  149.11798495719077\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  25 \t loss:  147.34171088118552\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  50 \t loss:  145.9088121803721\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  75 \t loss:  148.99087312417674\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  32 \t epoch:  100 \t loss:  144.18019612499197\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  25 \t loss:  147.41551368651128\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  50 \t loss:  149.5057239601355\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  75 \t loss:  149.72898775731588\n",
      "lr:  1e-05 \t alpha:  0.1 \t batch_size:  64 \t epoch:  100 \t loss:  149.0211728112342\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  25 \t loss:  217.81992205295714\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  50 \t loss:  198.47016234209852\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  75 \t loss:  183.17824698855011\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  8 \t epoch:  100 \t loss:  168.30325547800422\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  25 \t loss:  219.34835566932168\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  50 \t loss:  213.83895090760876\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  75 \t loss:  201.37469712124908\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  16 \t epoch:  100 \t loss:  193.80449067511995\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  25 \t loss:  229.2146512403019\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  50 \t loss:  221.1712073347489\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  75 \t loss:  219.57231284294576\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  32 \t epoch:  100 \t loss:  212.87474797146797\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  25 \t loss:  235.02412455622243\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  50 \t loss:  227.03018576826145\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  75 \t loss:  219.39767662977445\n",
      "lr:  1e-05 \t alpha:  1 \t batch_size:  64 \t epoch:  100 \t loss:  225.7708666168936\n"
     ]
    }
   ],
   "source": [
    "best_model = [w,b]\n",
    "best_loss = 1e5;\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "#                 define a new model\n",
    "                w = np.random.random([48*48])\n",
    "                b = np.random.rand()\n",
    "                model = [w,b]\n",
    "                batches = (int)(len(x_train)/batch_size)\n",
    "#                 perform training\n",
    "                for _ in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = x_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                #         print(len(data_x))\n",
    "                        prediction = forward_pass(data_x, model)\n",
    "                        loss = calculate_loss(prediction, data_y)\n",
    "                        regularization_loss = calculate_regularization(w,alpha)\n",
    "                        cost = loss + regularization_loss\n",
    "                #         print(cost)\n",
    "                        model = backprop(prediction, data_x, data_y, model, alpha, lr)\n",
    "#                 evaluate a model\n",
    "                loss = evaluate(x_validation, y_validation, model, alpha)\n",
    "                print(\"lr: \", lr, \"\\t alpha: \", alpha, \"\\t batch_size: \", batch_size, \"\\t epoch: \", epoch, \"\\t loss: \", loss)\n",
    "#                 save best model\n",
    "                if(loss<best_loss):\n",
    "                    best_loss = loss\n",
    "                    best_model = model\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.43569139946035\n"
     ]
    }
   ],
   "source": [
    "prediction = forward_pass(x_test, best_model)\n",
    "loss = calculate_loss(prediction, y_test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed form solution to linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION FOR REGRESSION PROBLEM\n",
    "def one_shot_linear_regression(X_tr, y_tr):\n",
    "    w = np.dot(np.linalg.inv(np.dot(X_tr.T,X_tr)),np.dot(X_tr.T,y_tr)) \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_age_regressor():\n",
    "    X_train = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48))   \n",
    "    y_train = np.load(\"age_regression_ytr.npy\")    \n",
    "    X_test = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))    \n",
    "    y_test = np.load(\"age_regression_yte.npy\")    \n",
    "\n",
    "    W = one_shot_linear_regression(X_train, y_train)\n",
    "\n",
    "    # prediction for training data\n",
    "    y_hat_train = np.dot(X_train,W) \n",
    "    # prediction for testing data\n",
    "    y_hat_test = np.dot(X_test,W) \n",
    "\n",
    "    # MSE for trainig error\n",
    "    train_error = 0.5*np.mean((y_hat_train-y_train)**2)\n",
    "    # MSE for testing error \n",
    "    test_error = 0.5*np.mean((y_hat_test-y_test)**2) \n",
    "\n",
    "    print(\"Training Error:\",train_error)\n",
    "    print(\"Testing Error:\",test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 50.46755488028351\n",
      "Testing Error: 269.1481156684566\n"
     ]
    }
   ],
   "source": [
    "train_age_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46751042957078004"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
