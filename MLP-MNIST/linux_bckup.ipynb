{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardcoded.ipynb                mnist_train_images.npy\r\n",
      "MNIST-2-layer.-reference.ipynb mnist_train_labels.npy\r\n",
      "linux_bckup.ipynb              mnist_validation_images.npy\r\n",
      "mnist_test_images.npy          mnist_validation_labels.npy\r\n",
      "mnist_test_labels.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"mnist_train_images.npy\")\n",
    "y_train = np.load(\"mnist_train_labels.npy\")\n",
    "X_test = np.load(\"mnist_test_images.npy\")\n",
    "y_test = np.load(\"mnist_test_labels.npy\")\n",
    "X_validation = np.load(\"mnist_validation_images.npy\")\n",
    "y_validation = np.load(\"mnist_validation_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random([X_train.shape[1], y_train.shape[1]])\n",
    "b = np.random.random([1,y_train.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    \"w\": w,\n",
    "    \"b\": b\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "#     z -= (np.max(z)/2)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "\n",
    "def forward_pass(x, model):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    z = (x@w+b)\n",
    "    a = softmax(z).T\n",
    "#     y = np.argmax(a, axis=1)\n",
    "    return a\n",
    "\n",
    "def calculate_CE_loss(a, one_hot_y):\n",
    "    cost = (-1 / a.shape[0]) * np.sum(one_hot_y * np.log(a))\n",
    "    return cost\n",
    "\n",
    "def calculate_regularization(w,alpha):\n",
    "    REGULARIZATION = 0.5 * alpha * (w.T@w) \n",
    "    return REGULARIZATION\n",
    "def backprop(prediction, x, y, model, alpha=0.01, lr=0.00001):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    grad_w = (-1/(x.shape[0]))*(x.T@(y-prediction)) + alpha * w\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = -1*np.mean(y-prediction, axis = 0)\n",
    "    b = b - lr*grad_b\n",
    "    model[\"w\"] = w\n",
    "    model[\"b\"] = b\n",
    "    return model\n",
    "def evaluate(x = X_validation, y = y_validation, model=model, alpha=0.1):\n",
    "    prediction = forward_pass(x, model)\n",
    "    CE_loss = calculate_CE_loss(prediction, y)\n",
    "    w= model[\"w\"]\n",
    "    reg_loss = calculate_regularization(w,alpha)\n",
    "    return CE_loss+reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_true):\n",
    "    return (sum(y == y_true)/len(y_true)) * 100\n",
    "# Hyperparameters - GRID SEARCH\n",
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [10]\n",
    "epochs = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    activated = np.maximum(0, x) #just for clarification\n",
    "    return activated\n",
    "def reluDerivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "batch_size = 1000\n",
    "data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "#     z -= np.max(z)\n",
    "    z = (np.array(z).T - np.max(z, axis = 1)).T\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "# def softmax(x):\n",
    "#     \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return (e_x / e_x.sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_neurons_1 = 75\n",
    "w1 = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "b1 = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "many_neurons_2 = 50\n",
    "w2 = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "b2 = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "# many_neurons_2 = 50\n",
    "w3 = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "b3 = np.random.random([1,data_y.shape[1]])/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hidden layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_neurons_1 = 75\n",
    "w1 = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "b1 = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "many_neurons_2 = 50\n",
    "w2 = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "b2 = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "# many_neurons_2 = 50\n",
    "w3 = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "b3 = np.random.random([1,data_y.shape[1]])/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [100]\n",
    "epochs = [200]\n",
    "\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "                for _ in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                        z1 = np.dot(data_x,w1)+b1\n",
    "                        a1 = relu(z1)\n",
    "                        z2 = np.dot(a1,w2)+b2\n",
    "                        a2 = relu(z2)\n",
    "                        z3 = np.dot(a2,w3)+b3\n",
    "                        a3 = softmax(z3).T\n",
    "\n",
    "                        batch = len(data_x)\n",
    "                        g = (np.subtract(a3,data_y))\n",
    "                        db3 = np.mean(g, axis = 0) \n",
    "                        dw3 = (1/batch)* np.dot(a2.T,g)\n",
    "                        dw3 += alpha * w3\n",
    "\n",
    "                        g = np.multiply(np.dot(g,w3.T),reluDerivative(z2))\n",
    "                        db2 = np.mean(g, axis = 0)\n",
    "                        dw2 = (1/batch)* np.dot(a1.T,g)\n",
    "                        dw2 += alpha * w2\n",
    "                        \n",
    "                        g = np.multiply(np.dot(g,w2.T),reluDerivative(z1))\n",
    "                        db1 = np.mean(g, axis = 0)\n",
    "                        dw1 = (1/batch)* np.dot(data_x.T,g)\n",
    "                        dw1 +=  alpha * w1\n",
    "                        \n",
    "                        lr = 1e-2\n",
    "                        w3 -= lr*dw3\n",
    "                        w2 -= lr*dw2\n",
    "                        w1 -= lr*dw1\n",
    "\n",
    "                        b3 -= lr*db3\n",
    "                        b2 -= lr*db2\n",
    "                        b1 -= lr*db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data - check accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z1 = np.dot(X_validation,w1)+b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1,w2)+b2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2,w3)+b3\n",
    "a3 = softmax(z3).T\n",
    "output = np.argmax(a3, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data - check accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z1 = np.dot(X_test,w1)+b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1,w2)+b2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2,w3)+b3\n",
    "a3 = softmax(z3).T\n",
    "output = np.argmax(a3, axis=1)\n",
    "actual_y = np.argmax(y_test, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer - How to do it??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size = 784, output_size=10, many_hidden_layers=4):\n",
    "    params = {}\n",
    "    nodes_in_layers = [75, 50, 25, 45, 30, 25, 20]\n",
    "    for layer in range(many_hidden_layers+1):\n",
    "        if(layer==0):\n",
    "            params[\"w\" + str(layer)]=np.random.random([input_size, nodes_in_layers[layer]])/20        \n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"w\" + str(layer)]=np.random.random([nodes_in_layers[layer-1], output_size])/20\n",
    "        else:\n",
    "            params[\"w\" + str(layer)] = np.random.random([nodes_in_layers[layer-1], nodes_in_layers[layer]])/20\n",
    "\n",
    "        if(layer==0):\n",
    "            params[\"b\" + str(layer)]=np.random.random([1, nodes_in_layers[layer]])/20\n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"b\" + str(layer)]=np.random.random([1, output_size])/20\n",
    "        else:\n",
    "            params[\"b\" + str(layer)] = np.random.random([1, nodes_in_layers[layer]])/20\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size = 784, output_size=10, many_hidden_layers=4):\n",
    "    params = {}\n",
    "    nodes_in_layers = [75, 50, 25, 45, 30, 25, 20]\n",
    "    for layer in range(many_hidden_layers+1):\n",
    "        if(layer==0):\n",
    "            c = nodes_in_layers[layer+1] ** -0.5\n",
    "            params[\"w\" + str(layer)]=np.random.uniform(-1*c/2,c/2,(input_size, nodes_in_layers[layer]))\n",
    "            \n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            c = nodes_in_layers[layer+1] ** -0.5\n",
    "            params[\"w\" + str(layer)]=np.random.uniform(-1*c/2,c/2,(nodes_in_layers[layer-1], output_size))\n",
    "        else:\n",
    "            c = nodes_in_layers[layer+1] ** -0.5\n",
    "            params[\"w\" + str(layer)]=np.random.uniform(-1*c/2,c/2,(nodes_in_layers[layer-1], nodes_in_layers[layer]))\n",
    "\n",
    "        if(layer==0):\n",
    "            params[\"b\" + str(layer)]=np.full([1, nodes_in_layers[layer]], 0.1)\n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"b\" + str(layer)]=np.full([1, output_size], 0.1) \n",
    "        else:\n",
    "            params[\"b\" + str(layer)] = np.full([1, nodes_in_layers[layer]], 0.1)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass with unknown depth\n",
    "# hey bhagwan!!\n",
    "def forward_pass(data_x, params, many_hidden_layers):\n",
    "    layer_cache = {}\n",
    "    z_current = data_x\n",
    "    for i in range(many_hidden_layers+1):\n",
    "    #     affine transformation\n",
    "        z_current = np.dot(z_current,params[\"w\" + str(i)]) + params[\"b\" + str(i)]\n",
    "    #     activation \n",
    "        if(i == many_hidden_layers):\n",
    "            a_current = softmax(z_current).T\n",
    "        else:\n",
    "            a_current = relu(z_current)\n",
    "#         print(\"z mean = \", z_current.mean())\n",
    "#         print(\"a mean = \",a_current.mean())\n",
    "        layer_cache[\"z\" + str(i)] = z_current;\n",
    "        layer_cache[\"a\" + str(i)] = a_current;\n",
    "    return layer_cache\n",
    "\n",
    "# x_current = softmax(x_current).T\n",
    "# output = np.argmax(x_current, axis=1)\n",
    "# actual_y = np.argmax(y_validation, axis=1)\n",
    "# validation_accuracy = accuracy(output,actual_y)\n",
    "# print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001):\n",
    "    derivatives = {}\n",
    "    batch = len(data_x)\n",
    "    for i in reversed(range(many_hidden_layers+1)):\n",
    "        if(i==many_hidden_layers):\n",
    "            g = (np.subtract(layer_cache[\"a\" + str(i)],data_y))\n",
    "        else:\n",
    "            g = np.multiply(np.dot(g,params[\"w\" + str(i+1)].T),reluDerivative(layer_cache[\"z\" + str(i)]))\n",
    "        derivatives[\"b\" + str(i)] = np.mean(g, axis = 0) \n",
    "        if(i==0):\n",
    "            derivatives[\"w\" + str(i)] = (1/batch)* np.dot(data_x.T,g)\n",
    "            derivatives[\"w\" + str(i)] += alpha * params[\"w\" + str(i)]\n",
    "        else:\n",
    "            derivatives[\"w\" + str(i)] = (1/batch)* np.dot(layer_cache[\"a\" + str(i-1)].T,g)\n",
    "            derivatives[\"w\" + str(i)] += alpha * params[\"w\" + str(i)]\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights\n",
    "\n",
    "def update_weights(params, derivatives, many_hidden_layers, prev_derivatives=derivatives,lr = 1e-3):\n",
    "    for i in range(many_hidden_layers+1):\n",
    "        params[\"w\" + str(i)] -= lr*derivatives[\"w\" + str(i)]\n",
    "        params[\"b\" + str(i)] -= lr*derivatives[\"b\" + str(i)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X, y,params, many_hidden_layers):\n",
    "    layer_cache = forward_pass(X, params, many_hidden_layers)\n",
    "    y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "    # x_current = softmax(y_hat).T\n",
    "    output = np.argmax(y_hat, axis=1)\n",
    "    actual_y = np.argmax(y, axis=1)\n",
    "    accuracy_val = accuracy(output,actual_y)\n",
    "    return accuracy_val  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params = init_params(input_size = 784, output_size=10, many_hidden_layers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_hidden_layers = 4\n",
    "for _ in range(100000):\n",
    "    layer_cache = forward_pass(data_x, params, many_hidden_layers)\n",
    "    derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001)\n",
    "    params = update_weights(params, derivatives, many_hidden_layers,lr = 1e-2)\n",
    "\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(data_y, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUGAAD for limited layers finding derivatives\n",
    "# derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001)\n",
    "def calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001):\n",
    "    batch = len(data_x)\n",
    "    i = 2\n",
    "    derivatives = {}\n",
    "    g = (np.subtract(layer_cache[\"a\"+str(i)],data_y))\n",
    "    derivatives[\"b\" + str(i)] = np.mean(g, axis = 0) \n",
    "    derivatives[\"w\" + str(i)] = (1/batch)* np.dot(layer_cache[\"a\"+str(i-1)].T,g)\n",
    "    derivatives[\"w\" + str(i)] += alpha * params[\"w\"+str(i)]\n",
    "\n",
    "\n",
    "    i = 1\n",
    "    g = np.multiply(np.dot(g,params[\"w\"+str(i+1)].T),reluDerivative(layer_cache[\"z\"+str(i)]))\n",
    "    derivatives[\"b\" + str(i)] = np.mean(g, axis = 0)\n",
    "    derivatives[\"w\" + str(i)] = (1/batch)* np.dot(layer_cache[\"a\"+str(i-1)].T,g)\n",
    "    derivatives[\"w\" + str(i)] += alpha * params[\"w\"+str(i)]\n",
    "\n",
    "    i = 0\n",
    "    g = np.multiply(np.dot(g,params[\"w\"+str(i+1)].T),reluDerivative(layer_cache[\"z\"+str(i)]))\n",
    "    derivatives[\"b\" + str(i)] = np.mean(g, axis = 0)\n",
    "    derivatives[\"w\" + str(i)] = (1/batch)* np.dot(data_x.T,g)\n",
    "    derivatives[\"w\" + str(i)] +=  alpha * params[\"w\"+str(i)]\n",
    "    return derivatives\n",
    "# update weights\n",
    "\n",
    "def update_weights(params, derivatives, many_hidden_layers,lr = 1e-2):\n",
    "    for i in range(many_hidden_layers+1):\n",
    "        params[\"w\" + str(i)] -= lr*derivatives[\"w\" + str(i)]\n",
    "        params[\"b\" + str(i)] -= lr*derivatives[\"b\" + str(i)]\n",
    "    return params\n",
    "def init_params(input_size = 784, output_size=10, many_hidden_layers=4):\n",
    "    nodes_in_layers = [75, 50, 40, 30, 25, 20, 15]\n",
    "    many_neurons_1 = 50\n",
    "    layer = 0;\n",
    "    params[\"w\" + str(layer)] = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "    params[\"b\" + str(layer)] = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "    layer = 1;\n",
    "    many_neurons_2 = 50\n",
    "    params[\"w\" + str(layer)] = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "    params[\"b\" + str(layer)] = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "    layer = 2;\n",
    "    # many_neurons_2 = 50\n",
    "    params[\"w\" + str(layer)] = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "    params[\"b\" + str(layer)] = np.random.random([1,data_y.shape[1]])/10\n",
    "    return params\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "#     z = (np.array(z).T - np.max(z, axis = 1)).T\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "# def forward_pass(data_x, params, many_hidden_layers):\n",
    "#     layer_cache = {}\n",
    "#     a_current = data_x\n",
    "#     for i in range(many_hidden_layers):\n",
    "#         z_current = np.dot(a_current,params[\"w\" + str(i)]) + params[\"b\" + str(i)]\n",
    "#         a_current = relu(z_current)\n",
    "#         layer_cache[\"z\" + str(i)] = z_current;\n",
    "#         layer_cache[\"a\" + str(i)] = a_current;\n",
    "#     q = many_hidden_layers\n",
    "#     z_current = np.dot(a_current,params[\"w\" + str(q)]) + params[\"b\" + str(q)]\n",
    "#     a_current = softmax(z_current).T\n",
    "#     layer_cache[\"z\" + str(q)] = z_current;\n",
    "#     layer_cache[\"a\" + str(q)] = a_current;\n",
    "#     return layer_cache\n",
    "def forward_pass(data_x, params, many_hidden_layers):\n",
    "    layer_cache = {}\n",
    "    i = 0\n",
    "    layer_cache[\"z\" + str(i)] = np.dot(data_x,params[\"w\" + str(i)])+params[\"b\" + str(i)]\n",
    "    layer_cache[\"a\" + str(i)] = relu(layer_cache[\"z\" + str(i)])\n",
    "    i = 1\n",
    "    layer_cache[\"z\" + str(i)] = np.dot(layer_cache[\"a\" + str(i-1)],params[\"w\" + str(i)])+params[\"b\" + str(i)]\n",
    "    layer_cache[\"a\" + str(i)] = relu(layer_cache[\"z\" + str(i)])\n",
    "    i = 2\n",
    "    layer_cache[\"z\" + str(i)] = np.dot(layer_cache[\"a\" + str(i-1)],params[\"w\" + str(i)])+params[\"b\" + str(i)]\n",
    "    layer_cache[\"a\" + str(i)] = softmax(layer_cache[\"z\" + str(i)]).T\n",
    "    return layer_cache\n",
    "# def init_params(input_size = 784, output_size=10, many_hidden_layers=4):\n",
    "#     params = {}\n",
    "#     many_neurons_1 = 75\n",
    "#     i = 0\n",
    "#     params[\"w\" + str(i)] = np.random.random([input_size, many_neurons_1])/10\n",
    "#     params[\"b\" + str(i)] = np.random.random([1,many_neurons_1])/10\n",
    "#     i = 1\n",
    "#     many_neurons_2 = 50\n",
    "#     params[\"w\" + str(i)] = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "#     params[\"b\" + str(i)] = np.random.random([1,many_neurons_2])/10\n",
    "#     i = 2\n",
    "#     # many_neurons_2 = 50\n",
    "#     params[\"w\" + str(i)] = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "#     params[\"b\" + str(i)] = np.random.random([1,output_size])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hidden_layers = 2\n",
    "params =init_params(input_size = 784, output_size=10, many_hidden_layers=many_hidden_layers)\n",
    "train_acc_epoch = []\n",
    "val_acc_epoch = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best acc = 10.02  @  0\n",
      "best acc = 19.06  @  1\n",
      "best acc = 33.800000000000004  @  2\n",
      "best acc = 44.54  @  3\n",
      "best acc = 52.6  @  4\n",
      "best acc = 59.56  @  5\n",
      "best acc = 69.16  @  6\n",
      "best acc = 74.96000000000001  @  7\n",
      "best acc = 77.16  @  8\n",
      "best acc = 78.08  @  9\n",
      "best acc = 78.94  @  10\n",
      "best acc = 79.62  @  11\n",
      "best acc = 80.06  @  12\n",
      "best acc = 80.66  @  13\n",
      "best acc = 81.0  @  14\n",
      "best acc = 81.22  @  15\n",
      "best acc = 81.42  @  16\n",
      "best acc = 81.66  @  17\n",
      "best acc = 81.88  @  18\n",
      "best acc = 81.92  @  19\n",
      "best acc = 82.0  @  20\n",
      "best acc = 82.04  @  21\n",
      "best acc = 82.1  @  22\n",
      "best acc = 82.22  @  23\n",
      "best acc = 82.26  @  24\n",
      "best acc = 82.36  @  25\n",
      "best acc = 82.38  @  26\n",
      "best acc = 82.42  @  30\n",
      "best acc = 82.44  @  31\n",
      "best acc = 82.5  @  32\n",
      "best acc = 82.58  @  33\n",
      "best acc = 82.69999999999999  @  34\n",
      "best acc = 82.8  @  35\n",
      "best acc = 82.86  @  37\n",
      "best acc = 82.92  @  38\n",
      "best acc = 82.94  @  44\n",
      "best acc = 83.0  @  46\n",
      "best acc = 83.04  @  47\n"
     ]
    }
   ],
   "source": [
    "lrs = [3.3e-3]\n",
    "alphas = [0.05]\n",
    "batch_sizes = [50]\n",
    "epochs = [50]\n",
    "# epoch = 60\n",
    "best_accuracy = 0;\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "                for cur_epoch in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                        layer_cache = forward_pass(data_x, params, many_hidden_layers)\n",
    "                        derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = alpha)\n",
    "                        params = update_weights(params, derivatives, many_hidden_layers,lr = lr)\n",
    "\n",
    "                    train_acc = calculate_accuracy(data_x, data_y,params, many_hidden_layers)\n",
    "                    val_acc = calculate_accuracy(X_validation, y_validation,params, many_hidden_layers)\n",
    "                    train_acc_epoch = np.append(train_acc_epoch, train_acc)\n",
    "                    val_acc_epoch = np.append(val_acc_epoch, val_acc)\n",
    "                    if(val_acc>best_accuracy):\n",
    "                        best_accuracy = val_acc\n",
    "                        print(\"best acc =\", val_acc,\" @ \", cur_epoch)\n",
    "                #print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, len(val_acc_epoch),len(val_acc_epoch))\n",
    "plt.plot(x, val_acc_epoch,'-', label=\"validation\")\n",
    "plt.plot(x, train_acc_epoch,'--', label=\"train\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylim(0, 110)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.32"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(X_test, y_test,params, many_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.76\n"
     ]
    }
   ],
   "source": [
    "layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10194213, -0.01195478,  0.03922353, ...,  0.04119096,\n",
       "        -0.01976633,  0.00063907],\n",
       "       [ 0.02103306,  0.05833214,  0.02538046, ...,  0.01762809,\n",
       "         0.02981448,  0.00351028],\n",
       "       [ 0.04568727, -0.02085826, -0.0127703 , ..., -0.01540765,\n",
       "        -0.02444906,  0.00578733],\n",
       "       ...,\n",
       "       [ 0.08433523,  0.00961607,  0.04051312, ...,  0.04035895,\n",
       "        -0.00447297,  0.0009995 ],\n",
       "       [-0.02394386,  0.10627409,  0.01373619, ...,  0.00610433,\n",
       "         0.06861395,  0.00576608],\n",
       "       [ 0.08997332, -0.05825473,  0.03089847, ...,  0.08263235,\n",
       "        -0.01367402, -0.00417524]])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"w1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.30363228e-08, 3.00482769e-07, 3.44325760e-07, ...,\n",
       "        4.26094088e-07, 2.10437857e-08, 1.53813265e-07],\n",
       "       [6.29070804e-08, 4.89103863e-07, 3.71565101e-07, ...,\n",
       "        2.12693558e-07, 2.87903270e-07, 5.26319921e-07],\n",
       "       [3.86422062e-07, 3.30723221e-07, 3.82347322e-07, ...,\n",
       "        4.97614257e-07, 5.36100882e-07, 4.86199296e-07],\n",
       "       ...,\n",
       "       [2.37017437e-07, 1.90986024e-07, 6.30915507e-08, ...,\n",
       "        5.39785744e-07, 4.29632716e-07, 5.26962466e-07],\n",
       "       [3.00041591e-07, 2.05229139e-07, 1.96571014e-07, ...,\n",
       "        5.36561223e-07, 1.50894557e-07, 3.09616915e-07],\n",
       "       [6.70588630e-08, 7.43051869e-08, 5.47903237e-07, ...,\n",
       "        1.26095723e-07, 5.56454092e-07, 3.37906551e-07]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivatives[\"w0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
