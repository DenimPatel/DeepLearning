{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST-2-layer.-reference.ipynb mnist_train_images.npy\r\n",
      "linux_bckup.ipynb              mnist_train_labels.npy\r\n",
      "mnist_test_images.npy          mnist_validation_images.npy\r\n",
      "mnist_test_labels.npy          mnist_validation_labels.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"mnist_train_images.npy\")\n",
    "y_train = np.load(\"mnist_train_labels.npy\")\n",
    "X_test = np.load(\"mnist_test_images.npy\")\n",
    "y_test = np.load(\"mnist_test_labels.npy\")\n",
    "X_validation = np.load(\"mnist_validation_images.npy\")\n",
    "y_validation = np.load(\"mnist_validation_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random([X_train.shape[1], y_train.shape[1]])\n",
    "b = np.random.random([1,y_train.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    \"w\": w,\n",
    "    \"b\": b\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "#     z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "\n",
    "def forward_pass(x, model):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    z = (x@w+b)\n",
    "    a = softmax(z).T\n",
    "#     y = np.argmax(a, axis=1)\n",
    "    return a\n",
    "\n",
    "def calculate_CE_loss(a, one_hot_y):\n",
    "    cost = (-1 / a.shape[0]) * np.sum(one_hot_y * np.log(a))\n",
    "    return cost\n",
    "\n",
    "def calculate_regularization(w,alpha):\n",
    "    REGULARIZATION = 0.5 * alpha * (w.T@w) \n",
    "    return REGULARIZATION\n",
    "def backprop(prediction, x, y, model, alpha=0.01, lr=0.00001):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    grad_w = (-1/(x.shape[0]))*(x.T@(y-prediction)) + alpha * w\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = -1*np.mean(y-prediction, axis = 0)\n",
    "    b = b - lr*grad_b\n",
    "    model[\"w\"] = w\n",
    "    model[\"b\"] = b\n",
    "    return model\n",
    "def evaluate(x = X_validation, y = y_validation, model=model, alpha=0.1):\n",
    "    prediction = forward_pass(x, model)\n",
    "    CE_loss = calculate_CE_loss(prediction, y)\n",
    "    w= model[\"w\"]\n",
    "    reg_loss = calculate_regularization(w,alpha)\n",
    "    return CE_loss+reg_loss\n",
    "def accuracy(y, y_true):\n",
    "    return (sum(y == y_true)/len(y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - GRID SEARCH\n",
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [10]\n",
    "epochs = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    activated = np.maximum(0, x) #just for clarification\n",
    "    return activated\n",
    "def reluDerivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "batch_size = 1000\n",
    "data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_neurons_1 = 75\n",
    "w1 = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "b1 = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "many_neurons_2 = 50\n",
    "w2 = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "b2 = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "# many_neurons_2 = 50\n",
    "w3 = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "b3 = np.random.random([1,data_y.shape[1]])/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hidden layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_neurons_1 = 75\n",
    "w1 = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "b1 = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "many_neurons_2 = 50\n",
    "w2 = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "b2 = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "# many_neurons_2 = 50\n",
    "w3 = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "b3 = np.random.random([1,data_y.shape[1]])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [100]\n",
    "epochs = [200]\n",
    "\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "                for _ in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                        z1 = np.dot(data_x,w1)+b1\n",
    "                        a1 = relu(z1)\n",
    "                        z2 = np.dot(a1,w2)+b2\n",
    "                        a2 = relu(z2)\n",
    "                        z3 = np.dot(a2,w3)+b3\n",
    "                        a3 = softmax(z3).T\n",
    "\n",
    "                        batch = len(data_x)\n",
    "                        g = (np.subtract(a3,data_y))\n",
    "                        db3 = np.mean(g, axis = 0) \n",
    "                        dw3 = (1/batch)* np.dot(a2.T,g)\n",
    "                        dw3 += alpha * w3\n",
    "\n",
    "                        g = np.multiply(np.dot(g,w3.T),reluDerivative(z2))\n",
    "                        db2 = np.mean(g, axis = 0)\n",
    "                        dw2 = (1/batch)* np.dot(a1.T,g)\n",
    "                        dw2 += alpha * w2\n",
    "                        \n",
    "                        g = np.multiply(np.dot(g,w2.T),reluDerivative(z1))\n",
    "                        db1 = np.mean(g, axis = 0)\n",
    "                        dw1 = (1/batch)* np.dot(data_x.T,g)\n",
    "                        dw1 +=  alpha * w1\n",
    "                        \n",
    "                        lr = 1e-2\n",
    "                        w3 -= lr*dw3\n",
    "                        w2 -= lr*dw2\n",
    "                        w1 -= lr*dw1\n",
    "\n",
    "                        b3 -= lr*db3\n",
    "                        b2 -= lr*db2\n",
    "                        b1 -= lr*db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data - check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.dot(X_validation,w1)+b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1,w2)+b2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2,w3)+b3\n",
    "a3 = softmax(z3).T\n",
    "output = np.argmax(a3, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data - check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.dot(X_test,w1)+b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1,w2)+b2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2,w3)+b3\n",
    "a3 = softmax(z3).T\n",
    "output = np.argmax(a3, axis=1)\n",
    "actual_y = np.argmax(y_test, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer - How to do it??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size = 784, output_size=10, many_hidden_layers=4):\n",
    "    params = {}\n",
    "    nodes_in_layers = [50, 40, 25, 45, 30, 25, 20]\n",
    "    for layer in range(many_hidden_layers+1):\n",
    "        if(layer==0):\n",
    "            params[\"w\" + str(layer)]=np.random.random([input_size, nodes_in_layers[layer]])/20        \n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"w\" + str(layer)]=np.random.random([nodes_in_layers[layer-1], output_size])/20\n",
    "        else:\n",
    "            params[\"w\" + str(layer)] = np.random.random([nodes_in_layers[layer-1], nodes_in_layers[layer]])/20\n",
    "\n",
    "        if(layer==0):\n",
    "            params[\"b\" + str(layer)]=np.random.random([1, nodes_in_layers[layer]])/20\n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"b\" + str(layer)]=np.random.random([1, output_size])/20\n",
    "        else:\n",
    "            params[\"b\" + str(layer)] = np.random.random([1, nodes_in_layers[layer]])/20\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass with unknown depth\n",
    "# hey bhagwan!!\n",
    "def forward_pass(data_x, params, many_hidden_layers):\n",
    "    layer_cache = {}\n",
    "    z_current = data_x\n",
    "    for i in range(many_hidden_layers+1):\n",
    "    #     affine transformation\n",
    "        z_current = np.dot(z_current,params[\"w\" + str(i)]) + params[\"b\" + str(i)]\n",
    "    #     activation \n",
    "        if(i == many_hidden_layers):\n",
    "            a_current = softmax(z_current).T\n",
    "        else:\n",
    "            a_current = relu(z_current)\n",
    "#         print(\"z mean = \", z_current.mean())\n",
    "#         print(\"a mean = \",a_current.mean())\n",
    "        layer_cache[\"z\" + str(i)] = z_current;\n",
    "        layer_cache[\"a\" + str(i)] = a_current;\n",
    "    return layer_cache\n",
    "\n",
    "# x_current = softmax(x_current).T\n",
    "# output = np.argmax(x_current, axis=1)\n",
    "# actual_y = np.argmax(y_validation, axis=1)\n",
    "# validation_accuracy = accuracy(output,actual_y)\n",
    "# print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001):\n",
    "    derivatives = {}\n",
    "    batch = len(data_x)\n",
    "    for i in reversed(range(many_hidden_layers+1)):\n",
    "        if(i==many_hidden_layers):\n",
    "            g = (np.subtract(layer_cache[\"a\" + str(i)],data_y))\n",
    "        else:\n",
    "            g = np.multiply(np.dot(g,params[\"w\" + str(i+1)].T),reluDerivative(layer_cache[\"z\" + str(i)]))\n",
    "        derivatives[\"b\" + str(i)] = np.mean(g, axis = 0) \n",
    "        if(i==0):\n",
    "            derivatives[\"w\" + str(i)] = (1/batch)* np.dot(data_x.T,g)\n",
    "            derivatives[\"w\" + str(i)] += alpha * params[\"w\" + str(i)]\n",
    "        else:\n",
    "            derivatives[\"w\" + str(i)] = (1/batch)* np.dot(layer_cache[\"a\" + str(i-1)].T,g)\n",
    "            derivatives[\"w\" + str(i)] += alpha * params[\"w\" + str(i)]\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights\n",
    "\n",
    "def update_weights(params, derivatives, many_hidden_layers,lr = 1e-3):\n",
    "    for i in range(many_hidden_layers+1):\n",
    "        params[\"w\" + str(i)] -= lr*derivatives[\"w\" + str(i)]\n",
    "        params[\"b\" + str(i)] -= lr*derivatives[\"b\" + str(i)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X, y,params, many_hidden_layers):\n",
    "    layer_cache = forward_pass(X, params, many_hidden_layers)\n",
    "    y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "    # x_current = softmax(y_hat).T\n",
    "    output = np.argmax(y_hat, axis=1)\n",
    "    actual_y = np.argmax(y, axis=1)\n",
    "    accuracy_val = accuracy(output,actual_y)\n",
    "    return accuracy_val  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params = init_params(input_size = 784, output_size=10, many_hidden_layers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_hidden_layers = 4\n",
    "for _ in range(100000):\n",
    "    layer_cache = forward_pass(data_x, params, many_hidden_layers)\n",
    "    derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001)\n",
    "    params = update_weights(params, derivatives, many_hidden_layers,lr = 1e-2)\n",
    "\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(data_y, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hidden_layers = 1\n",
    "params =init_params(input_size = 784, output_size=10, many_hidden_layers=many_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best acc = 9.82  @  0\n",
      "best acc = 15.28  @  1\n",
      "best acc = 32.56  @  2\n",
      "best acc = 57.52  @  3\n",
      "best acc = 68.47999999999999  @  4\n",
      "best acc = 71.12  @  5\n",
      "best acc = 73.64  @  6\n",
      "best acc = 75.56  @  7\n",
      "best acc = 77.60000000000001  @  8\n",
      "best acc = 79.46  @  9\n",
      "best acc = 81.04  @  10\n",
      "best acc = 82.02000000000001  @  11\n",
      "best acc = 83.12  @  12\n",
      "best acc = 83.82  @  13\n",
      "best acc = 84.6  @  14\n",
      "best acc = 85.22  @  15\n",
      "best acc = 85.82  @  16\n",
      "best acc = 86.26  @  17\n",
      "best acc = 86.58  @  18\n",
      "best acc = 87.0  @  19\n",
      "best acc = 87.28  @  20\n",
      "best acc = 87.56  @  21\n",
      "best acc = 87.72  @  22\n",
      "best acc = 87.94  @  23\n",
      "best acc = 88.14  @  24\n",
      "best acc = 88.34  @  25\n",
      "best acc = 88.52  @  26\n",
      "best acc = 88.7  @  27\n",
      "best acc = 88.75999999999999  @  28\n",
      "best acc = 88.86  @  29\n",
      "best acc = 88.94  @  30\n",
      "best acc = 89.03999999999999  @  31\n",
      "best acc = 89.16  @  32\n",
      "best acc = 89.34  @  33\n",
      "best acc = 89.46  @  34\n",
      "best acc = 89.56  @  35\n",
      "best acc = 89.72  @  36\n",
      "best acc = 89.84  @  37\n",
      "best acc = 89.96  @  38\n",
      "best acc = 90.02  @  39\n",
      "best acc = 90.22  @  40\n",
      "best acc = 90.32  @  42\n",
      "best acc = 90.36  @  46\n",
      "best acc = 90.38000000000001  @  47\n",
      "best acc = 90.42  @  49\n",
      "best acc = 90.56  @  50\n",
      "best acc = 90.62  @  51\n",
      "best acc = 90.68  @  52\n",
      "best acc = 90.72  @  58\n",
      "best acc = 90.74  @  59\n",
      "best acc = 90.78  @  60\n",
      "best acc = 90.8  @  61\n",
      "best acc = 90.86  @  62\n",
      "best acc = 90.94  @  63\n",
      "best acc = 91.0  @  64\n",
      "best acc = 91.03999999999999  @  65\n",
      "best acc = 91.08000000000001  @  67\n",
      "best acc = 91.12  @  69\n",
      "best acc = 91.14  @  70\n",
      "best acc = 91.2  @  71\n",
      "best acc = 91.22  @  72\n",
      "best acc = 91.24  @  73\n",
      "best acc = 91.25999999999999  @  74\n",
      "best acc = 91.3  @  75\n",
      "best acc = 91.32000000000001  @  76\n",
      "best acc = 91.42  @  77\n",
      "best acc = 91.46  @  79\n",
      "best acc = 91.47999999999999  @  82\n",
      "best acc = 91.56  @  83\n",
      "best acc = 91.62  @  84\n",
      "best acc = 91.64  @  85\n",
      "best acc = 91.67999999999999  @  88\n",
      "best acc = 91.72  @  90\n",
      "best acc = 91.74  @  91\n",
      "best acc = 91.75999999999999  @  92\n",
      "best acc = 91.82000000000001  @  93\n",
      "best acc = 91.84  @  94\n",
      "best acc = 91.88  @  95\n",
      "best acc = 91.9  @  98\n",
      "best acc = 91.92  @  99\n",
      "best acc = 91.97999999999999  @  100\n",
      "best acc = 92.0  @  101\n",
      "best acc = 92.08  @  106\n",
      "best acc = 92.10000000000001  @  107\n",
      "best acc = 92.12  @  108\n",
      "best acc = 92.14  @  110\n",
      "best acc = 92.17999999999999  @  111\n",
      "best acc = 92.2  @  116\n",
      "best acc = 92.22  @  118\n",
      "best acc = 92.25999999999999  @  124\n",
      "best acc = 92.28  @  125\n",
      "best acc = 92.32000000000001  @  130\n",
      "best acc = 92.34  @  133\n",
      "best acc = 92.36  @  137\n"
     ]
    }
   ],
   "source": [
    "lrs = [3.3e-3]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [200]\n",
    "epochs = [150]\n",
    "best_accuracy = 0;\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "                for cur_epoch in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                        layer_cache = forward_pass(data_x, params, many_hidden_layers)\n",
    "                        derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001)\n",
    "                        params = update_weights(params, derivatives, many_hidden_layers,lr = 3.3e-3)\n",
    "\n",
    "                    val_acc = calculate_accuracy(X_validation, y_validation,params, many_hidden_layers)\n",
    "                    if(val_acc>best_accuracy):\n",
    "                        best_accuracy = val_acc\n",
    "                        print(\"best acc =\", val_acc,\" @ \", cur_epoch)\n",
    "                #print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_accuracy(X_test, y_test,params, many_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives[\"w3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
