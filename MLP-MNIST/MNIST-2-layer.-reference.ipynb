{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST-2-layer.-reference.ipynb mnist_train_labels.npy\r\n",
      "mnist_test_images.npy          mnist_validation_images.npy\r\n",
      "mnist_test_labels.npy          mnist_validation_labels.npy\r\n",
      "mnist_train_images.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"mnist_train_images.npy\")\n",
    "y_train = np.load(\"mnist_train_labels.npy\")\n",
    "X_test = np.load(\"mnist_test_images.npy\")\n",
    "y_test = np.load(\"mnist_test_labels.npy\")\n",
    "X_validation = np.load(\"mnist_validation_images.npy\")\n",
    "y_validation = np.load(\"mnist_validation_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random([X_train.shape[1], y_train.shape[1]])\n",
    "b = np.random.random([1,y_train.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "model = {\n",
    "    \"w\": w,\n",
    "    \"b\": b\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "#     z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "\n",
    "def forward_pass(x, model):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    z = (x@w+b)\n",
    "    a = softmax(z).T\n",
    "#     y = np.argmax(a, axis=1)\n",
    "    return a\n",
    "\n",
    "def calculate_CE_loss(a, one_hot_y):\n",
    "    cost = (-1 / a.shape[0]) * np.sum(one_hot_y * np.log(a))\n",
    "    return cost\n",
    "\n",
    "def calculate_regularization(w,alpha):\n",
    "    REGULARIZATION = 0.5 * alpha * (w.T@w) \n",
    "    return REGULARIZATION\n",
    "def backprop(prediction, x, y, model, alpha=0.01, lr=0.00001):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    grad_w = (-1/(x.shape[0]))*(x.T@(y-prediction)) + alpha * w\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = -1*np.mean(y-prediction, axis = 0)\n",
    "    b = b - lr*grad_b\n",
    "    model[\"w\"] = w\n",
    "    model[\"b\"] = b\n",
    "    return model\n",
    "def evaluate(x = X_validation, y = y_validation, model=model, alpha=0.1):\n",
    "    prediction = forward_pass(x, model)\n",
    "    CE_loss = calculate_CE_loss(prediction, y)\n",
    "    w= model[\"w\"]\n",
    "    reg_loss = calculate_regularization(w,alpha)\n",
    "    return CE_loss+reg_loss\n",
    "def accuracy(y, y_true):\n",
    "    return (sum(y == y_true)/len(y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - GRID SEARCH\n",
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [10]\n",
    "epochs = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    activated = np.maximum(0, x) #just for clarification\n",
    "    return activated\n",
    "def reluDerivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "batch_size = 1000\n",
    "data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_neurons_1 = 75\n",
    "w1 = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "b1 = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "many_neurons_2 = 50\n",
    "w2 = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "b2 = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "# many_neurons_2 = 50\n",
    "w3 = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "b3 = np.random.random([1,data_y.shape[1]])/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hidden layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_neurons_1 = 75\n",
    "w1 = np.random.random([data_x.shape[1], many_neurons_1])/10\n",
    "b1 = np.random.random([1,many_neurons_1])/10\n",
    "\n",
    "many_neurons_2 = 50\n",
    "w2 = np.random.random([many_neurons_1, many_neurons_2])/10\n",
    "b2 = np.random.random([1,many_neurons_2])/10\n",
    "\n",
    "# many_neurons_2 = 50\n",
    "w3 = np.random.random([many_neurons_2, data_y.shape[1]])/10\n",
    "b3 = np.random.random([1,data_y.shape[1]])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [100]\n",
    "epochs = [200]\n",
    "\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "                for _ in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                        z1 = np.dot(data_x,w1)+b1\n",
    "                        a1 = relu(z1)\n",
    "                        z2 = np.dot(a1,w2)+b2\n",
    "                        a2 = relu(z2)\n",
    "                        z3 = np.dot(a2,w3)+b3\n",
    "                        a3 = softmax(z3).T\n",
    "\n",
    "                        batch = len(data_x)\n",
    "                        g = (np.subtract(a3,data_y))\n",
    "                        db3 = np.mean(g, axis = 0) \n",
    "                        dw3 = (1/batch)* np.dot(a2.T,g)\n",
    "                        dw3 += alpha * w3\n",
    "\n",
    "                        g = np.multiply(np.dot(g,w3.T),reluDerivative(z2))\n",
    "                        db2 = np.mean(g, axis = 0)\n",
    "                        dw2 = (1/batch)* np.dot(a1.T,g)\n",
    "                        dw2 += alpha * w2\n",
    "                        \n",
    "                        g = np.multiply(np.dot(g,w2.T),reluDerivative(z1))\n",
    "                        db1 = np.mean(g, axis = 0)\n",
    "                        dw1 = (1/batch)* np.dot(data_x.T,g)\n",
    "                        dw1 +=  alpha * w1\n",
    "                        \n",
    "                        lr = 1e-2\n",
    "                        w3 -= lr*dw3\n",
    "                        w2 -= lr*dw2\n",
    "                        w1 -= lr*dw1\n",
    "\n",
    "                        b3 -= lr*db3\n",
    "                        b2 -= lr*db2\n",
    "                        b1 -= lr*db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data - check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.dot(X_validation,w1)+b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1,w2)+b2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2,w3)+b3\n",
    "a3 = softmax(z3).T\n",
    "output = np.argmax(a3, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data - check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.dot(X_test,w1)+b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1,w2)+b2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2,w3)+b3\n",
    "a3 = softmax(z3).T\n",
    "output = np.argmax(a3, axis=1)\n",
    "actual_y = np.argmax(y_test, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer - How to do it??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size = 784, output_size=10, many_hidden_layers=4):\n",
    "    params = {}\n",
    "    nodes_in_layers = [60, 50, 55, 45, 30, 25, 20]\n",
    "    for layer in range(many_hidden_layers+1):\n",
    "        if(layer==0):\n",
    "            params[\"w\" + str(layer)]=np.random.random([input_size, nodes_in_layers[layer]])/20        \n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"w\" + str(layer)]=np.random.random([nodes_in_layers[layer-1], output_size])/20\n",
    "        else:\n",
    "            params[\"w\" + str(layer)] = np.random.random([nodes_in_layers[layer-1], nodes_in_layers[layer]])/20\n",
    "\n",
    "        if(layer==0):\n",
    "            params[\"b\" + str(layer)]=np.random.random([1, nodes_in_layers[layer]])/20\n",
    "        elif(layer==(many_hidden_layers)):\n",
    "            params[\"b\" + str(layer)]=np.random.random([1, output_size])/20\n",
    "        else:\n",
    "            params[\"b\" + str(layer)] = np.random.random([1, nodes_in_layers[layer]])/20\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass with unknown depth\n",
    "# hey bhagwan!!\n",
    "def forward_pass(data_x, params, many_hidden_layers):\n",
    "    layer_cache = {}\n",
    "    z_current = data_x\n",
    "    for i in range(many_hidden_layers+1):\n",
    "    #     affine transformation\n",
    "        z_current = np.dot(z_current,params[\"w\" + str(i)]) + params[\"b\" + str(i)]\n",
    "    #     activation \n",
    "        if(i == many_hidden_layers):\n",
    "            a_current = softmax(z_current).T\n",
    "        else:\n",
    "            a_current = relu(z_current)\n",
    "#         print(\"z mean = \", z_current.mean())\n",
    "#         print(\"a mean = \",a_current.mean())\n",
    "        layer_cache[\"z\" + str(i)] = z_current;\n",
    "        layer_cache[\"a\" + str(i)] = a_current;\n",
    "    return layer_cache\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001):\n",
    "    derivatives = {}\n",
    "    batch = len(data_x)\n",
    "    for i in reversed(range(many_hidden_layers+1)):\n",
    "        if(i==many_hidden_layers):\n",
    "            g = (np.subtract(layer_cache[\"a\" + str(i)],data_y))\n",
    "        else:\n",
    "            g = np.multiply(np.dot(g,params[\"w\" + str(i+1)].T),reluDerivative(layer_cache[\"z\" + str(i)]))\n",
    "        derivatives[\"b\" + str(i)] = np.mean(g, axis = 0) \n",
    "        if(i==0):\n",
    "            derivatives[\"w\" + str(i)] = (1/batch)* np.dot(data_x.T,g)\n",
    "#             derivatives[\"w\" + str(i)] += alpha * params[\"w\" + str(i)]\n",
    "        else:\n",
    "            derivatives[\"w\" + str(i)] = (1/batch)* np.dot(layer_cache[\"a\" + str(i-1)].T,g)\n",
    "#             derivatives[\"w\" + str(i)] += alpha * params[\"w\" + str(i)]\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights\n",
    "\n",
    "def update_weights(params, derivatives, many_hidden_layers,lr = 1e-3):\n",
    "    for i in range(many_hidden_layers+1):\n",
    "        params[\"w\" + str(i)] -= lr*derivatives[\"w\" + str(i)]\n",
    "        params[\"b\" + str(i)] -= lr*derivatives[\"b\" + str(i)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_params(input_size = 784, output_size=10, many_hidden_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.9\n"
     ]
    }
   ],
   "source": [
    "many_hidden_layers = 4\n",
    "for _ in range(1000):\n",
    "    layer_cache = forward_pass(data_x, params, many_hidden_layers)\n",
    "    derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.001)\n",
    "    params = update_weights(params, derivatives, many_hidden_layers,lr = 1e-2)\n",
    "\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(data_y, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.82\n"
     ]
    }
   ],
   "source": [
    "layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_params(input_size = 784, output_size=10, many_hidden_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.08\n"
     ]
    }
   ],
   "source": [
    "lrs = [3.3e-3]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [100]\n",
    "epochs = [26]\n",
    "\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "                for _ in range(epoch):\n",
    "                    for i in range(batches):\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "                        layer_cache = forward_pass(data_x, params, many_hidden_layers)\n",
    "                        derivatives = calculate_derivatives(data_x, data_y,params, layer_cache, many_hidden_layers, alpha = 0.0)\n",
    "                        params = update_weights(params, derivatives, many_hidden_layers,lr = 3.3e-3)\n",
    "\n",
    "                layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "                y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "                # x_current = softmax(y_hat).T\n",
    "                output = np.argmax(y_hat, axis=1)\n",
    "                actual_y = np.argmax(y_validation, axis=1)\n",
    "                validation_accuracy = accuracy(output,actual_y)\n",
    "                print(validation_accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.58\n"
     ]
    }
   ],
   "source": [
    "layer_cache = forward_pass(X_validation, params, many_hidden_layers)\n",
    "y_hat = layer_cache[\"a\"+str(many_hidden_layers)]\n",
    "# x_current = softmax(y_hat).T\n",
    "output = np.argmax(y_hat, axis=1)\n",
    "actual_y = np.argmax(y_validation, axis=1)\n",
    "validation_accuracy = accuracy(output,actual_y)\n",
    "print(validation_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
