{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfomance on validation\n",
      "lr:  0.033 \t alpha:  0.001 \t batch_size:  400 \t epoch:  500\n",
      "\t val_acc:  92.44 \t val_loss  1.5780613221798316\n",
      "\n",
      "test accuracy=  92.13 test loss =  0.28232217000674603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHyper parameters that we found\\nlearning rate: 0.033\\nalpha: 0.001\\nbatch_size: 400\\nepoch: 500\\ntest_accuray: 91.66\\ntest_loss: 0.2963\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = np.load(\"mnist_train_images.npy\")\n",
    "y_train = np.load(\"mnist_train_labels.npy\")\n",
    "X_test = np.load(\"mnist_test_images.npy\")\n",
    "y_test = np.load(\"mnist_test_labels.npy\")\n",
    "X_validation = np.load(\"mnist_validation_images.npy\")\n",
    "y_validation = np.load(\"mnist_validation_labels.npy\")\n",
    "w = np.random.random([X_train.shape[1], y_train.shape[1]])\n",
    "b = np.random.random([1,y_train.shape[1]])\n",
    "model = {\n",
    "    \"w\": w,\n",
    "    \"b\": b\n",
    "}    \n",
    "def softmax(z):\n",
    "#     z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "\n",
    "def forward_pass(x, model):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    z = (x@w+b)\n",
    "    a = softmax(z).T\n",
    "#     y = np.argmax(a, axis=1)\n",
    "    return a\n",
    "\n",
    "def calculate_CE_loss(a, one_hot_y):\n",
    "    cost = (-1 / a.shape[0]) * np.sum(one_hot_y * np.log(a))\n",
    "    return cost\n",
    "\n",
    "def calculate_regularization(w,alpha):\n",
    "    REGULARIZATION = 0.5 * alpha * (w.T@w) \n",
    "    return REGULARIZATION\n",
    "def backprop(prediction, x, y, model, alpha=0.01, lr=0.00001):\n",
    "    w= model[\"w\"]\n",
    "    b= model[\"b\"]\n",
    "    grad_w = (-1/(x.shape[0]))*(x.T@(y-prediction)) + alpha * w\n",
    "    w = w - lr*grad_w\n",
    "    grad_b = -1*np.mean(y-prediction, axis = 0)\n",
    "    b = b - lr*grad_b\n",
    "    model[\"w\"] = w\n",
    "    model[\"b\"] = b\n",
    "    return model\n",
    "def evaluate(x = X_validation, y = y_validation, model=model, alpha=0.1):\n",
    "    prediction = forward_pass(x, model)\n",
    "    CE_loss = calculate_CE_loss(prediction, y)\n",
    "    w= model[\"w\"]\n",
    "    reg_loss = calculate_regularization(w,alpha)\n",
    "    return CE_loss+reg_loss\n",
    "def accuracy(y, y_true):\n",
    "    return (sum(y == y_true)/len(y_true)) * 100\n",
    "\n",
    "# Hyperparameters - GRID SEARCH\n",
    "lrs = [3.3e-2]\n",
    "alphas = [0.001]\n",
    "batch_sizes = [400]\n",
    "epochs = [500]\n",
    "\n",
    "best_loss = 1e5\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    for alpha in alphas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                w = np.random.random([X_train.shape[1], y_train.shape[1]])\n",
    "                b = np.random.random([1,y_train.shape[1]])\n",
    "                model = {\n",
    "                    \"w\": w,\n",
    "                    \"b\":b}  \n",
    "                batches = (int)(len(X_train)/batch_size)\n",
    "#                 print(batches)\n",
    "                for _ in range(epoch):\n",
    "                    for i in range(batches):\n",
    "    #                     print(i)\n",
    "                        data_x, data_y = X_train[(i)*batch_size:(i)*batch_size+batch_size,], y_train[(i)*batch_size:(i)*batch_size+batch_size,]\n",
    "#                         print(len(data_x))\n",
    "                        a = forward_pass(data_x, model)\n",
    "    #                     CE_loss = calculate_CE_loss(a, data_y)\n",
    "    #             #         regularization_loss = calculate_regularization(w,alpha)\n",
    "    #             #         cost = CE_loss + regularization_loss\n",
    "    #             #         print(cost)\n",
    "                        output = np.argmax(a, axis=1)\n",
    "                        actual_y = np.argmax(data_y, axis=1)\n",
    "                #         print(\"train accuracy= \",accuracy(output,actual_y),\"train loss = \", calculate_loss(a, data_y))\n",
    "                        model = backprop(a, data_x, data_y, model, alpha, lr)\n",
    "\n",
    "                 #                 save best model\n",
    "\n",
    "            print(\"Perfomance on validation\")\n",
    "            a = forward_pass(X_validation, model)\n",
    "            output = np.argmax(a, axis=1)\n",
    "            actual_y = np.argmax(y_validation, axis=1)\n",
    "            validation_accuracy = accuracy(output,actual_y)\n",
    "            valdiation_loss = calculate_CE_loss(a, y_validation)+calculate_regularization(w.ravel(),alpha) \n",
    "            print(\"lr: \", lr, \"\\t alpha: \", alpha, \"\\t batch_size: \", batch_size, \"\\t epoch: \", epoch)\n",
    "            print(\"\\t val_acc: \",validation_accuracy,\"\\t val_loss \", valdiation_loss)\n",
    "            print(\"\")\n",
    "            if(validation_accuracy>best_accuracy):\n",
    "                best_accuracy = validation_accuracy\n",
    "                best_model = model\n",
    "                best_params = {\n",
    "                    \"lr\" : lr,\n",
    "                    \"alpha\": alpha,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"epoch\":epoch\n",
    "                }\n",
    "\n",
    "                \n",
    "## for testing the best model\n",
    "a = forward_pass(X_test, best_model)\n",
    "output = np.argmax(a, axis=1)\n",
    "actual_y = np.argmax(y_test, axis=1)\n",
    "print(\"test accuracy= \",accuracy(output,actual_y),\"test loss = \", calculate_CE_loss(a, y_test))\n",
    "\n",
    "\n",
    "'''\n",
    "Hyper parameters that we found\n",
    "learning rate: 0.033\n",
    "alpha: 0.001\n",
    "batch_size: 400\n",
    "epoch: 500\n",
    "test_accuray: 91.66\n",
    "test_loss: 0.2963\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
